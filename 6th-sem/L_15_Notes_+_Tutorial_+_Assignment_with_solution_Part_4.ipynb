{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L_15_Notes_+_Tutorial_+_Assignment_with_solution_Part_4.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"15yeoGWcfpy4"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"VAQQX5uCHg9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter 15\n","\n","---\n","\n","\n","\n","# Introduction to Natural Language Processing\n","\n","In this chapter, you will be getting to know the basic elements in Natural Language Processing. You will be studying different topics in 5 parts, \n","- Part 1: Here, you will take text and get to its basic components using processes such as Tokenization,  Lemmatization, Stop words, Stemming, Part of speech tagging and Text Normalization\n","- Part 2: Using the basic omponents figured in the part 1 you will extract basic important words using Named entity Recognition and then learn to create Mathematical representation of texts to use them in machine learning models.\n","- Part 3: Using certain algorithms next you will explore word and grammar autocorrection, Word similarity, Key term detection and Extractive text summary\n","- Part 4: In this part you will move to training deep learning algorithms for Text sentiment analysis and Text classification, Text emotion detection\n","- Part 5: Finally, you will lean about natural language generation, training a simple chatbot and then merging computer vision and natural language processing with Image caption generation"],"metadata":{"id":"wtYEj3DEp5_t"}},{"cell_type":"code","source":[""],"metadata":{"id":"JGUPs3N6p4Py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What lies ahead for you in this chapter\n","\n","In the first part you learnt about\n","- Tokenization\n","- Finding out stop words\n","- Lemmatization\n","- Stemming\n","- Part of speech tagging\n","- Morphology\n","- Named Entity Recognition\n","- Basic features and mathematical representation of text\n","\n","In the previous part you were introduced to topics of \n","- Text spell correction\n","- Grammar correction\n","- Word similarity check\n","- Sentence similarity check\n","- Key terms extraction\n","- Text summary extraction\n","- Text Sentiment Analysis \n","\n","\n","In this chapter you will be \n","- training your own custom Text classification systems, \n","- with a demo on spam detection. \n","- As assignments you will be working on training an Emotion recognition system and sentiment analysis system, and a fake news detection system. \n","\n","\n","*Note: Since this is an introductory chapter to NLP, you wont be getting deep mathematical understanding of concepts, and its impossible to cover every algorithm or a concept too. So you will be getting to know what exists in this field, intuitive understanding and the codes to work your way round these concepts and experiment with small little applications in this field.* "],"metadata":{"id":"8rnT1OQJp-Yp"}},{"cell_type":"code","source":[""],"metadata":{"id":"aboauaSwHg5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_4U9CjdT1FaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What is Natural Language Processing?\n","\n","It falls in the field of computer science, linguistics, and artificial intelligence that focusses on understand human language, interpret it as we do and generate response in a manner as humans do. Afterall, all the sci-fi movies have shown that the AI speaks very similar to humans. \n","\n","When we speak with each other, we understand and interpret every statement with context. For example, \n","- Statement 1: It took me two hours to travel a mere 2 kms yesterday evening\n","- Statement 2: It was horrible, my friend fell sick as well.\n","If I ask you, are these statements connected? TYou would say yes, try this with Alexa or google assistant or Siri, and check if they are able to connect the two statements and repond accordingly. \n","\n","NLP combined rules based linguistic modelling like rules of grammar, language, etc; and deep learning algorithms involving LSTMs, RNNs, etc. NLP has two parts\n","- Natural Language Understanding\n","  - Taking up human language, converting it into mathematical format, extracting meaning and key information from it.\n","- Natural Language Generation\n","  - Based on the understanding of the extracted information, the ai engine generates natural language with proper grammar and in a way we can understand it.\n","\n","\n","The basic steps in the Natural language understanding involve\n","- Lexical analysis: Splitting your text paragraph into sentences and each sentence into words.\n","- Syntactic analysis: This stage include analysis of each of these words, their meaning and their arrangements in the sentences, basically understanding the relationships\n","- Semantic analysis: This phase tries to understand meaningfulness in the sentence. \n","- Discourse integration: In this part, the context is extracted, and relationship is drawn between multiple sentences.\n","- Pragmatic analysis: This phase is a double check on intention of the text. Basically texts can have multiple meanings,\n","  - Ravi saw Suraj in front of his house with a binoculars \n","    - In this sentence, is Ravi having those binoculars or is Suraj having it. \n","  - Or is a question asked direct or rhetorical? \n","\n","\n","There are multiple applications of natural language processing\n","- Email and sms filters\n","- Smart AI chatbot assistants\n","- Making search results faster in case of huge documents\n","- Finding similarity between legal documents and cases\n","- Sentiment and emotion analysis of texts and statements\n","- Auto-summarization of large texts\n","- Text auto-correct and auto-suggest\n","... and much more!!!\n","\n","\n","\n"],"metadata":{"id":"On_PShu31FuD"}},{"cell_type":"code","source":[""],"metadata":{"id":"nTOr90ka1FXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HvgaWn5HHg2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spacy Library\n","\n","[Spacy](https://spacy.io/) is an Industrial strength library for Natural Language Processing, released under [MIT License](https://github.com/explosion/spaCy/blob/master/LICENSE). \n","\n","![](https://drive.google.com/uc?export=view&id=1H4e3LBNkBlx8x67Is7QaCjEoeCff9XzF)\n","\n","It is one of the most used libraries in this space, developed and managed by an AI company named, [Explosion AI](https://explosion.ai/)"],"metadata":{"id":"DHg7vt3Uwg28"}},{"cell_type":"code","source":[""],"metadata":{"id":"wQ8j0jDDvp4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install spacy"],"metadata":{"id":"yjov0eDW0Ozf"}},{"cell_type":"code","source":["! pip install -U pip setuptools wheel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"iiaeCmPCvp07","executionInfo":{"status":"ok","timestamp":1652179961215,"user_tz":-330,"elapsed":12353,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"ddecd03a-98bf-4fd9-8a31-c5d1a641e6d6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 48.8 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Installing collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pip-22.0.4 setuptools-62.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}}]},{"cell_type":"code","source":["! pip uninstall -y spacy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsL_95MX0fX5","executionInfo":{"status":"ok","timestamp":1652179976810,"user_tz":-330,"elapsed":2694,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"8c719f5e-9c3f-4690-95a5-f3abf28fd881"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: spacy 2.2.4\n","Uninstalling spacy-2.2.4:\n","  Successfully uninstalled spacy-2.2.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["! pip install spacy==3.2.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVHbYRP40fTv","executionInfo":{"status":"ok","timestamp":1652179995094,"user_tz":-330,"elapsed":18290,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"c6675fd2-ff14-4da3-d660-0ba469652e62"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting spacy==3.2.4\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (62.1.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.23.0)\n","Collecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Collecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.11.3)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (7.1.2)\n","Collecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.1/457.1 KB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (3.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (0.9.1)\n","Collecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (0.4.1)\n","Collecting typing-extensions<4.0.0.0,>=3.7.4\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (21.3)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (1.0.7)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (4.64.0)\n","Collecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 KB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.3/653.3 KB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m76.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.0.6)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (1.21.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.4) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.4) (3.0.8)\n","Collecting smart-open<6.0.0,>=5.0.0\n","  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (3.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.4) (2.0.1)\n","Installing collected packages: typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, langcodes, catalogue, srsly, pathy, thinc, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.2.0\n","    Uninstalling typing_extensions-4.2.0:\n","      Successfully uninstalled typing_extensions-4.2.0\n","  Attempting uninstall: smart-open\n","    Found existing installation: smart-open 6.0.0\n","    Uninstalling smart-open-6.0.0:\n","      Successfully uninstalled smart-open-6.0.0\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["! python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EySGJfW0fQT","executionInfo":{"status":"ok","timestamp":1652180018719,"user_tz":-330,"elapsed":23632,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"00d4f0f8-2961-439f-912f-ca68f61bc996"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (62.1.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 2.2.5\n","    Uninstalling en-core-web-sm-2.2.5:\n","      Successfully uninstalled en-core-web-sm-2.2.5\n","Successfully installed en-core-web-sm-3.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["! python -m spacy download en_core_web_md"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4RNAPCw0fMN","executionInfo":{"status":"ok","timestamp":1652180034513,"user_tz":-330,"elapsed":15804,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"8314e779-8bf8-4956-a745-119e16fd3102"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-md==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.2.0) (3.2.4)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.7)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.7)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.15)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (62.1.0)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.2)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.64.0)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.6)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Dy-gWABV0fJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Cevx2vOGPBHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ACibjKmDzsE6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Text classification systems\n","\n","Like image classification, you can classify the texts as well. Classifying them makes it easier to extract information. For example, when you know a text is not spam, you can proceed ahead and get better information from it. Or, when you know the emotion from the text is say an \"angry\" one, then the response of a chatbot needs to be a very different one. \n","\n","Application involve\n","- SMS/Mail Spam detection\n","- Emotion detection from text\n","- Sentiment analysis from text\n","- Detecting urgency in text\n","- Automated customer response"],"metadata":{"id":"ecd0izkDIXsd"}},{"cell_type":"code","source":[""],"metadata":{"id":"vC2ZhqiDlIIh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OTJBpRJ2lIly"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Spam detection using classy classification\n","\n","[Classy classification](https://spacy.io/universe/project/classyclassification) is an opensource extnsion over Spacy's text categorizer which allows you to train a text classification system with less lines of codes. It is licensed as [MIT license](https://github.com/Pandora-Intelligence/classy-classification/blob/main/LICENSE).\n","\n","\n","It works with few shot classification using sentence transformers and spacy libraries at its core. Few shot classification is required when you have vew less data. It is aimed to make the AI engine learn with very few examples as reference. \n","\n","Reference material for few shot and zero shot learning\n","- https://aixplain.com/2021/09/23/zero-shot-learning-in-natural-language-processing/\n","- https://www.analyticsvidhya.com/blog/2021/05/an-introduction-to-few-shot-learning/\n","- https://analyticsindiamag.com/a-complete-tutorial-on-zero-shot-text-classification/\n","\n","You will train a spam detection engine using this library. \n","\n","![](https://drive.google.com/uc?export=view&id=1_OYrD3-b30jq2e30yLyvWZXNSXONQnKn)\n","\n","![](https://drive.google.com/uc?export=view&id=1Ksw9Xwm71zbA4r-bYbdXMGWfUjrYGTfJ)\n","\n"],"metadata":{"id":"EGUSeITLlI9c"}},{"cell_type":"code","source":[""],"metadata":{"id":"VUXpdzQZ0hJ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install classy-classification"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mbLK37OGKM5b","outputId":"5d1f3c83-ab0b-4a1b-a199-cdc4c54ffb99","executionInfo":{"status":"ok","timestamp":1652180065015,"user_tz":-330,"elapsed":30506,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting classy-classification\n","  Downloading classy_classification-0.4.1-py3-none-any.whl (13 kB)\n","Collecting sentence-transformers<3.0,>=2.0\n","  Downloading sentence-transformers-2.2.0.tar.gz (79 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.7/79.7 KB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: scikit-learn<2.0,>=1.0 in /usr/local/lib/python3.7/dist-packages (from classy-classification) (1.0.2)\n","Requirement already satisfied: spacy[transformers]<4.0,>=3.0 in /usr/local/lib/python3.7/dist-packages (from classy-classification) (3.2.4)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->classy-classification) (1.1.0)\n","Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->classy-classification) (1.21.6)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->classy-classification) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn<2.0,>=1.0->classy-classification) (1.4.1)\n","Collecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.18.0-py3-none-any.whl (4.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers<3.0,>=2.0->classy-classification) (4.64.0)\n","Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers<3.0,>=2.0->classy-classification) (1.11.0+cu113)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers<3.0,>=2.0->classy-classification) (0.12.0+cu113)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers<3.0,>=2.0->classy-classification) (3.2.5)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m56.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 KB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (3.0.6)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (0.4.1)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (3.10.0.2)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (8.0.15)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (2.0.7)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (3.0.9)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (1.0.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (2.11.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (0.9.1)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (2.0.6)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (7.1.2)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (0.4.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (62.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (21.3)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (1.8.2)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (0.6.1)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (3.3.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (1.0.7)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy[transformers]<4.0,>=3.0->classy-classification) (2.4.3)\n","Collecting spacy-transformers<1.2.0,>=1.1.2\n","  Downloading spacy_transformers-1.1.5-py2.py3-none-any.whl (51 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 KB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy[transformers]<4.0,>=3.0->classy-classification) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy[transformers]<4.0,>=3.0->classy-classification) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy[transformers]<4.0,>=3.0->classy-classification) (5.2.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]<4.0,>=3.0->classy-classification) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]<4.0,>=3.0->classy-classification) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]<4.0,>=3.0->classy-classification) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy[transformers]<4.0,>=3.0->classy-classification) (1.24.3)\n","Collecting transformers<5.0.0,>=4.6.0\n","  Downloading transformers-4.17.0-py3-none-any.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m80.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n","  Downloading spacy_alignments-0.8.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers<3.0,>=2.0->classy-classification) (2019.12.20)\n","Collecting tokenizers!=0.11.3,>=0.11.1\n","  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers<3.0,>=2.0->classy-classification) (4.11.3)\n","Collecting pyyaml>=5.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m596.3/596.3 KB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting sacremoses\n","  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 KB\u001b[0m \u001b[31m41.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers<3.0,>=2.0->classy-classification) (3.6.0)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy[transformers]<4.0,>=3.0->classy-classification) (2.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk->sentence-transformers<3.0,>=2.0->classy-classification) (1.15.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence-transformers<3.0,>=2.0->classy-classification) (7.1.2)\n","Building wheels for collected packages: sentence-transformers, sacremoses\n","  Building wheel for sentence-transformers (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.0-py3-none-any.whl size=120751 sha256=42c8fcee7ba1793c482c86ac865b854ada17ea92df90c14dfc5a55901cd8bb61\n","  Stored in directory: /root/.cache/pip/wheels/83/c0/df/b6873ab7aac3f2465aa9144b6b4c41c4391cfecc027c8b07e7\n","  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895260 sha256=8bd4105d29b0d0ea6ef15389df1b4634b38f772bc7932c4d28aa6fdcbf1cf6fe\n","  Stored in directory: /root/.cache/pip/wheels/87/39/dd/a83eeef36d0bf98e7a4d1933a4ad2d660295a40613079bafc9\n","Successfully built sentence-transformers sacremoses\n","Installing collected packages: tokenizers, sentencepiece, spacy-alignments, sacremoses, pyyaml, huggingface-hub, transformers, sentence-transformers, spacy-transformers, classy-classification\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","Successfully installed classy-classification-0.4.1 huggingface-hub-0.5.1 pyyaml-6.0 sacremoses-0.0.53 sentence-transformers-2.2.0 sentencepiece-0.1.96 spacy-alignments-0.8.5 spacy-transformers-1.1.5 tokenizers-0.12.1 transformers-4.17.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"LMQPOlE8lIiu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"I1K1rUD5IXNE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process\n","\n","The process is very simple and involves 4 steps\n","- Download the dataset\n","  - Fairly simple as the name goes, just make sure you check the license before downloading\n","\n","- Arrange the dataset\n","  - Arrangement has to be very simple\n","  - Two dictionaries\n","    - train_dict \n","    - test_dict\n","  - Each dictionary to have keys as classes and each key to have list of examples\n","  - Like,\n","    ```\n","    train_dict = {\n","          \"spam\": [\"You have won a 1000$ lottery\", \n","                  \"Your number ending with 5698 has won a 50EUR award\"],\n","          \"not_spam\": [\"I will be late for dinner, kelp 15$ dinner money on the table\",\n","                      \"Your doctor's appointment at Apollo is at 12 PM tomorrow\"]\n","      }\n","    ```\n","- Training step\n","  - Load the library and language model\n","  - Add the data to configuration pipeline of spacy nlp module \n","    - This will auto-train the data\n","- Inference step\n","  - Load a new sms text and see if its a spam or not\n"],"metadata":{"id":"451scwuBlcu_"}},{"cell_type":"code","source":[""],"metadata":{"id":"idlTP29q2L3p"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"uHZUYMZo2L0X"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"jhmPgqHi2LDt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Download the dataset\n","\n","SMS spam collection dataset resides here, https://archive-beta.ics.uci.edu/ml/datasets/sms+spam+collection, at UCI Machine Learning Repository. The data is licensed as [CC By 4.0](https://creativecommons.org/licenses/by/4.0/legalcode) hence can also be used for commercial purposes. \n","\n","As quoted on the website\n"," - It has some 425 Spam messages taken from Grumbletext Web site\n"," - And has some 3375 normal messages taken from NUS SMS Corpus"],"metadata":{"id":"n01KRHEhJX2o"}},{"cell_type":"code","source":["! wget https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SEcFCM_MJXon","outputId":"b4f0adfe-c6e0-4f90-da38-68dad08c1b38","executionInfo":{"status":"ok","timestamp":1652163099117,"user_tz":-330,"elapsed":830,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-10 06:11:39--  https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\n","Resolving archive.ics.uci.edu (archive.ics.uci.edu)... 128.195.10.252\n","Connecting to archive.ics.uci.edu (archive.ics.uci.edu)|128.195.10.252|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 203415 (199K) [application/x-httpd-php]\n","Saving to: ‘smsspamcollection.zip’\n","\n","smsspamcollection.z 100%[===================>] 198.65K  --.-KB/s    in 0.1s    \n","\n","2022-05-10 06:11:39 (1.73 MB/s) - ‘smsspamcollection.zip’ saved [203415/203415]\n","\n"]}]},{"cell_type":"code","source":["! unzip -qq smsspamcollection.zip"],"metadata":{"id":"AG_YxkftIXKN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["f = open(\"SMSSpamCollection\");\n","lines = f.readlines();\n","f.close();"],"metadata":{"id":"QCkDqOb1JpSW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lines[0].split(\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bnAeo0inJpOY","outputId":"fb18df61-038e-4a06-f85c-72dacc987c82","executionInfo":{"status":"ok","timestamp":1652163112966,"user_tz":-330,"elapsed":367,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['ham',\n"," 'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...\\n']"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["lines[2].split(\"\\t\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tAROs038JpF9","outputId":"c80f3240-09a5-486d-aad5-503ed7f6adc8","executionInfo":{"status":"ok","timestamp":1652163114601,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['spam',\n"," \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\\n\"]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":[""],"metadata":{"id":"e9BIXQ3UIXHC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"aTf9thHtmLe8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Arrange the dataset\n","\n","- Arrangement has to be very simple\n","  - Two dictionaries\n","    - train_dict \n","    - test_dict\n","  - Each dictionary to have keys as classes and each key to have list of examples\n","  - Like,\n","    ```\n","    train_dict = {\n","          \"spam\": [\"You have won a 1000$ lottery\", \n","                  \"Your number ending with 5698 has won a 50EUR award\"],\n","          \"not_spam\": [\"I will be late for dinner, kelp 15$ dinner money on the table\",\n","                      \"Your doctor's appointment at Apollo is at 12 PM tomorrow\"]\n","      }\n","    ```"],"metadata":{"id":"Xvu2MW9QmMo0"}},{"cell_type":"code","source":["spam = [];\n","not_spam = [];\n","\n","for i in range(len(lines)):\n","    label, text = lines[i].split(\"\\t\");\n","    if(label == \"spam\"):\n","        spam.append(text);\n","    else:\n","        not_spam.append(text);"],"metadata":{"id":"iE7zIpXmKNAR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(spam), len(not_spam)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ltQWaJJLVJ6","outputId":"ee8da9cd-dea8-4cba-9507-bedbd4e1e783","executionInfo":{"status":"ok","timestamp":1652163156623,"user_tz":-330,"elapsed":6,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(747, 4827)"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[""],"metadata":{"id":"U-wcDuYMLi0K"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_spam = [];\n","test_spam = [];\n","\n","for i in range(int(0.9*len(spam))):\n","    train_spam.append(spam[i]);\n","\n","for i in range(int(0.9*len(spam)), int(1.0*len(spam))):\n","    test_spam.append(spam[i]);"],"metadata":{"id":"OcFKa3yULVGO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9Jz5GXwxL5gg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_not_spam = [];\n","test_not_spam = [];\n","\n","for i in range(int(0.8*len(not_spam))):\n","    train_not_spam.append(not_spam[i]);\n","\n","for i in range(int(0.8*len(not_spam)), int(1.0*len(not_spam))):\n","    test_not_spam.append(not_spam[i]);"],"metadata":{"id":"t5pxCLuRLVCX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"J0yL3Fq8KM8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create training data\n","\n","data = {\n","        \"spam\": train_spam,\n","        \"not_spam\": train_not_spam\n","}"],"metadata":{"id":"DXFfgBPMeT94"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OPbnv1xceT7L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","- Load the library and language model\n","  - Add the data to configuration pipeline of spacy nlp module \n","    - This will auto-train the data"],"metadata":{"id":"qTJPQ9_YMKC9"}},{"cell_type":"code","source":["# Import library\n","\n","import spacy\n","import classy_classification"],"metadata":{"id":"ZZflOkIzLDPc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load language model\n","\n","nlp = spacy.load('en_core_web_md')"],"metadata":{"id":"EW-6mtr1LDMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bQovQKeeLDI_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ysULQK-_Medo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"p044E2xvNcs0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Add text categorizer pipeline with your data\n","# This may take a few minutes to train\n","\n","nlp.add_pipe(\"text_categorizer\", \n","    config={\n","        \"data\": data,\n","        \"model\": \"spacy\"\n","    }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jO7AeEXYLDGG","outputId":"af2068a4-b375-4fd1-e804-d41eb058e609","executionInfo":{"status":"ok","timestamp":1652163281194,"user_tz":-330,"elapsed":76319,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"]},{"output_type":"execute_result","data":{"text/plain":["<classy_classification.classifiers.spacy_internal.classySpacyInternal at 0x7f66fe6f4810>"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[""],"metadata":{"id":"YygT0ZQyKM2a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JHhghjtpM3Bk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing"],"metadata":{"id":"JR5Jp6vuM3T-"}},{"cell_type":"code","source":["sentence = test_spam[0];\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"id":"ICCsJxuLM2-j","outputId":"6709b1fc-2da0-4bca-9395-2437e875c7dd","executionInfo":{"status":"ok","timestamp":1652163368986,"user_tz":-330,"elapsed":384,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Phony £350 award - Todays Voda numbers ending XXXX are selected to receive a £350 award. If you have a match please call 08712300220 quoting claim code 3100 standard rates app\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":["doc = nlp(sentence)\n","print(doc._.cats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"glO_xy-oIXDd","outputId":"b8f79106-2e04-4d94-b2d1-4d917214a96c","executionInfo":{"status":"ok","timestamp":1652163370617,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'not_spam': 5.350259136709974e-06, 'spam': 0.9999946497408633}\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"LZNiRGvjN-JB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JPx7q-pzN_5x"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-NPO55RlN_2n"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = test_not_spam[1];\n","sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"outputId":"f1c695a9-750b-4738-8527-224b38f78929","id":"jYJlnMF8OCeL","executionInfo":{"status":"ok","timestamp":1652163380124,"user_tz":-330,"elapsed":7,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'Ranjith cal drpd Deeraj and deepak 5min hold\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["doc = nlp(sentence)\n","print(doc._.cats)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"255f1210-5bfa-4d12-fd31-c08bd67e3aee","id":"RhvaVP3JOCeN","executionInfo":{"status":"ok","timestamp":1652163387140,"user_tz":-330,"elapsed":585,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'not_spam': 0.9774266438625316, 'spam': 0.022573356137468317}\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"-o8SOSJAN_zI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"nkqZAXS0N_wO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7IL335D_Kcju"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Emotion Detection using textblob and spacy\n","\n","This becomes crucial in understanding human text in multiple places\n","- customer reviews and emotions\n","- devising next reply for a chatbot\n","\n","![](https://drive.google.com/uc?export=view&id=1gKN5MCZEQI70HtxMPgLOkQBs2N-Q1y5P)\n","\n","![](https://drive.google.com/uc?export=view&id=1dycJx1ZhEh_wXP2qu44x7Ny4lJtHq0gY)\n"],"metadata":{"id":"Ijh8bsirpbSD"}},{"cell_type":"code","source":[""],"metadata":{"id":"GLyQ-R-upay5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Q-wtDLmoS9uu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["[Textblob](https://github.com/sloria/TextBlob) is an opensource library for processing of text data licensed as [MIT License](https://github.com/sloria/TextBlob/blob/dev/LICENSE). "],"metadata":{"id":"yheSqYB3BmxM"}},{"cell_type":"code","source":[""],"metadata":{"id":"k-nJg42ABzox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install textblob"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StWShiDmBzjm","executionInfo":{"status":"ok","timestamp":1652174887412,"user_tz":-330,"elapsed":4924,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"0028097c-1527-47a1-ce37-86e1a94ad754"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"E4ocbi8HTZ64"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"s8a7T63TTZ36"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9drk21oTk3Iv","outputId":"b47b2a8e-fead-4388-8423-9f04c15c4bf7","executionInfo":{"status":"ok","timestamp":1652174998144,"user_tz":-330,"elapsed":4716,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import nltk\n","\n","# Doanload all language corpus data and models\n","# A corpus is a collection of documents\n","nltk.download(\"all\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTnY0u-OlVno","outputId":"ac35a7ac-52fe-431f-b526-25fb7cedc415","executionInfo":{"status":"ok","timestamp":1652175057186,"user_tz":-330,"elapsed":52984,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":[""],"metadata":{"id":"wFTgkuoiTCPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"r2lQ1SSnTgdP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Process\n","\n","The process is very simple and involves 4 steps\n","- Download the dataset\n","  - Fairly simple as the name goes, just make sure you check the license before downloading\n","\n","- Arrange the dataset\n","  - Arrangement has to be very simple\n","  - Two lists\n","    - train_list\n","    - test_list\n","  - Each list to have tuples with sentence and classes\n","  - Like,\n","    ```\n","    train_list = [\n","              (\"You have won a 1000$ lottery\", \"spam\"),\n","              (\"Your number ending with 5698 has won a 50EUR award\", \"spam\"),\n","              (\"I will be late for dinner, kelp 15$ dinner money on the table\", \"not_spam\"),\n","              (\"Your doctor's appointment at Apollo is at 12 PM tomorrow\", \"not_spam\")\n","    ]\n","    ```\n","- Training step\n","  - Load the library and select a classifier\n","  - Add the data to classifier and extract a trained modules\n","  \n","- Inference step\n","  - Load a new tweet and see the emotion using classify modules\n"],"metadata":{"id":"V0S_pLAnOIwv"}},{"cell_type":"code","source":[""],"metadata":{"id":"j128H6e-OIww"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"R2o4bRcVMLxm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Download the dataset\n","\n","Emotion detection dataset is at kaggle here, https://www.kaggle.com/datasets/pashupatigupta/emotion-detection-from-text, released with [CCO: Public domain license](https://creativecommons.org/publicdomain/zero/1.0/).\n","\n","The dataset has multiple (13) classes of emotions for some 10K+ tweets\n","- 'anger',\n","- 'boredom',\n","- 'empty',\n","- 'enthusiasm',\n","- 'fun',\n","- 'happiness',\n","- 'hate',\n","- 'love',\n","- 'neutral',\n","- 'relief',\n","- 'sadness',\n","- 'surprise',\n","- 'worry'\n","\n","\n","Kaggle is your go-to place to experiment on different datasets. It has more than 1000s of datasets including tabular ones, image and video datasets, and text datasets. This place will take you towards your goals of becoming a master of deep learning and maching learning. A bonus is, you will find codes of great developers and engineers there from which you can learn a lot, best practices, new algorithms, approaches, etc.\n","\n","Step 1: \n","Install Kaggle API using PIP. \n","```\n","pip install kaggle\n","```\n","\n","\n","Step 2:\n","-Create your account on [kaggle](https://www.kaggle.com/)\n","- Click on profile icon and then on account\n"," ![](https://drive.google.com/uc?export=view&id=1Q2agtXYXdYMMVOUXau-HvxanPe9xCAhz)\n","- Click on Create new API token\n"," ![](https://drive.google.com/uc?export=view&id=1H-MaQ77kukbQ0mUQqlvL25F15UOnJDfC) \n","\n","It will download a file named \"kaggle.json\" and you upload this here.\n","\n","\n"],"metadata":{"id":"Bqe57lNM_Knp"}},{"cell_type":"code","source":["! pip install kaggle"],"metadata":{"id":"_hHwuG5H_Knp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"execution":{"iopub.status.busy":"2022-03-01T16:51:15.473341Z","iopub.execute_input":"2022-03-01T16:51:15.473604Z","iopub.status.idle":"2022-03-01T16:51:25.927760Z","shell.execute_reply.started":"2022-03-01T16:51:15.473577Z","shell.execute_reply":"2022-03-01T16:51:25.926693Z"},"trusted":true,"id":"MvnkCE7O_Knp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JsoYhqoX_Knp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! mkdir ~/.kaggle"],"metadata":{"id":"M5bt5Z-p_Knp","executionInfo":{"status":"ok","timestamp":1652180065016,"user_tz":-330,"elapsed":11,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["! cp kaggle.json ~/.kaggle/"],"metadata":{"id":"0xdAg_4U_Knp","executionInfo":{"status":"ok","timestamp":1652180065016,"user_tz":-330,"elapsed":9,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["! kaggle datasets download pashupatigupta/emotion-detection-from-text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652180066254,"user_tz":-330,"elapsed":1246,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"dceb67c5-d214-404d-df51-6201baab225b","id":"Hgj-etcD_Knp"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /root/.kaggle/kaggle.json'\n","Downloading emotion-detection-from-text.zip to /content\n","  0% 0.00/1.56M [00:00<?, ?B/s]\n","100% 1.56M/1.56M [00:00<00:00, 151MB/s]\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"S0sxKNSGMLts"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Arrange the dataset\n","\n","- Arrangement has to be very simple\n","  - Two lists\n","    - train_list\n","    - test_list\n","  - Each list to have tuples with sentence and classes\n","  - Like,\n","    ```\n","    train_list = [\n","              (\"You have won a 1000$ lottery\", \"spam\"),\n","              (\"Your number ending with 5698 has won a 50EUR award\", \"spam\"),\n","              (\"I will be late for dinner, kelp 15$ dinner money on the table\", \"not_spam\"),\n","              (\"Your doctor's appointment at Apollo is at 12 PM tomorrow\", \"not_spam\")\n","    ]\n","    ```\n","\n","**Note: Textblob is very slow when training data is big, hence we will use just 5 tweets per class**"],"metadata":{"id":"ACNVXnZVVl6G"}},{"cell_type":"code","source":[""],"metadata":{"id":"x5nw6wsRKjAJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! unzip -qq emotion-detection-from-text.zip"],"metadata":{"id":"rM6TqwzsKcda","executionInfo":{"status":"ok","timestamp":1652180066255,"user_tz":-330,"elapsed":20,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IWUwEDj8V5jo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"tweet_emotions.csv\")"],"metadata":{"id":"L9AyxZi3V5f0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"OKLebOBSV5dE","executionInfo":{"status":"ok","timestamp":1652176842826,"user_tz":-330,"elapsed":20,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"be22f9fe-6924-422f-d115-f8ed677e4004"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["     tweet_id   sentiment                                            content\n","0  1956967341       empty  @tiffanylue i know  i was listenin to bad habi...\n","1  1956967666     sadness  Layin n bed with a headache  ughhhh...waitin o...\n","2  1956967696     sadness                Funeral ceremony...gloomy friday...\n","3  1956967789  enthusiasm               wants to hang out with friends SOON!\n","4  1956968416     neutral  @dannycastillo We want to trade with someone w..."],"text/html":["\n","  <div id=\"df-923bc5a3-f02c-40a1-aaa3-d2767c91c564\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-923bc5a3-f02c-40a1-aaa3-d2767c91c564')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-923bc5a3-f02c-40a1-aaa3-d2767c91c564 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-923bc5a3-f02c-40a1-aaa3-d2767c91c564');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":[""],"metadata":{"id":"O-wwwT7XV5af"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","class_list = list(np.unique(df[\"sentiment\"]));\n","class_list"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mWv-gWSfV5XZ","executionInfo":{"status":"ok","timestamp":1652176844729,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"073a24c8-d6e2-4394-ea0e-018e5a2a32dd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['anger',\n"," 'boredom',\n"," 'empty',\n"," 'enthusiasm',\n"," 'fun',\n"," 'happiness',\n"," 'hate',\n"," 'love',\n"," 'neutral',\n"," 'relief',\n"," 'sadness',\n"," 'surprise',\n"," 'worry']"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":[""],"metadata":{"id":"HXqftD-wV5Un"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_tweets = {};\n","\n","for i in range(len(df)):\n","    tweet, cls = df[\"content\"][i], df[\"sentiment\"][i];\n","    if cls not in class_tweets.keys():\n","        class_tweets[cls] = [tweet];\n","    else:\n","        class_tweets[cls].append(tweet)"],"metadata":{"id":"ieUGHXTYZ5Jr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"UMPZAGQLZ5Gt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_list = [];\n","test_list = [];\n","\n","for key in class_tweets.keys():\n","    for i in range(5):\n","        tmp = (class_tweets[key][i], key);\n","        train_list.append(tmp);\n","\n","for key in class_tweets.keys():\n","    for i in range(5):\n","        tmp = (class_tweets[key][i], key);\n","        test_list.append(tmp);"],"metadata":{"id":"IBwMpwZaa2MQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"-PmWPMy1a2Ir"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(train_list), len(test_list)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tUxAC52zW7wc","executionInfo":{"status":"ok","timestamp":1652177111237,"user_tz":-330,"elapsed":6,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"8d50ad19-200d-4c01-8f8e-1aed75246bd3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(65, 65)"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":[""],"metadata":{"id":"Db2h4HxmZFQn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"aS8I_XIFXd5i"}},{"cell_type":"code","source":["# Import libraries\n","from textblob import classifiers"],"metadata":{"id":"GHlfI-RTXaWp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"j23B0C2TXrVF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Select the classifier and start training\n","cls = classifiers.NaiveBayesClassifier(train_list)"],"metadata":{"id":"JE-dHDYvXrQe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"W2Akk60VXrNx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6WeKkuB3YJYv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing"],"metadata":{"id":"YjqvnL7UYJux"}},{"cell_type":"code","source":["tweet, actual_class = test_list[6]\n","predicted_class = cls.classify(tweet)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(predicted_class));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tlR-fMEWYJVr","executionInfo":{"status":"ok","timestamp":1652177312419,"user_tz":-330,"elapsed":372,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"bf2362ce-e5eb-474e-fc41-8fa30f668ef7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  Funeral ceremony...gloomy friday...\n","Actual class    :  sadness\n","Predicted class :  sadness\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"PE99JXh-cTTu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet, actual_class = test_list[12]\n","predicted_class = cls.classify(tweet)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(predicted_class));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4_KQl5sOcTPO","executionInfo":{"status":"ok","timestamp":1652177325659,"user_tz":-330,"elapsed":417,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"2a335601-1296-4af0-992f-a77d627e5ec9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  I want another tatt\n","Actual class    :  enthusiasm\n","Predicted class :  boredom\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"pSuX5ongcTL2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tweet, actual_class = test_list[34]\n","predicted_class = cls.classify(tweet)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(predicted_class));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fi20wA8dcWjX","executionInfo":{"status":"ok","timestamp":1652177341734,"user_tz":-330,"elapsed":416,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"fef60a4f-f475-4052-a82e-a7b909c6ae0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  @NisforNeemah thanks neemah. I'm gonna be soooo close to you and izzy, yet so far\n","Actual class    :  love\n","Predicted class :  love\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"VhT8gW7TXaQc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"kVPMAtHWzsBL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Assignment - Apprach it using Spacy and Classy Classifier\n","\n","On the same emotion detection dataset, use spacy to train your own classifier\n","\n","**Note: Use only a small part of the dataset for training for faster results**\n","\n","\n","**Additionally: Use preprocessing techniques to clean the dataset**"],"metadata":{"id":"uBYjmjBZbhcw"}},{"cell_type":"code","source":[""],"metadata":{"id":"FCXshFvFbhJs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"u0K4xOXVjF0Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VLeHV2XOjFxq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"0DAN0ITkkaQh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"VYIpmczukaHj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Arrange the dataset"],"metadata":{"id":"YsTmgNS3c4We"}},{"cell_type":"code","source":["import pandas as pd\n","\n","df = pd.read_csv(\"tweet_emotions.csv\")"],"metadata":{"id":"6rDnTqHtbhHG","executionInfo":{"status":"ok","timestamp":1652180066924,"user_tz":-330,"elapsed":680,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","\n","class_list = list(np.unique(df[\"sentiment\"]));\n","class_list"],"metadata":{"id":"qeZQOwzIbhEO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652180108250,"user_tz":-330,"elapsed":570,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"688d2b5d-9619-4930-f974-7f00c5ca5e17"},"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['anger',\n"," 'boredom',\n"," 'empty',\n"," 'enthusiasm',\n"," 'fun',\n"," 'happiness',\n"," 'hate',\n"," 'love',\n"," 'neutral',\n"," 'relief',\n"," 'sadness',\n"," 'surprise',\n"," 'worry']"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","source":["class_tweets = {};\n","\n","for i in range(len(df)):\n","    tweet, cls = df[\"content\"][i], df[\"sentiment\"][i];\n","    if cls not in class_tweets.keys():\n","        class_tweets[cls] = [tweet];\n","    else:\n","        class_tweets[cls].append(tweet)"],"metadata":{"id":"hr1QyzNNbhBB","executionInfo":{"status":"ok","timestamp":1652180112971,"user_tz":-330,"elapsed":1587,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"IC2prtY5dBKn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_class_tweets = {};\n","test_class_tweets = {};\n","\n","for key in class_tweets.keys():\n","    train_class_tweets[key] = [];\n","    test_class_tweets[key] = [];\n","\n","    for i in range(25):\n","        train_class_tweets[key].append(class_tweets[key][i]);\n","    for i in range(25, 50):\n","        test_class_tweets[key].append(class_tweets[key][i]);"],"metadata":{"id":"OAweT36AdBHg","executionInfo":{"status":"ok","timestamp":1652181581250,"user_tz":-330,"elapsed":345,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"yYuW4FxJdBD6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iu5tGmRLsRBg","executionInfo":{"status":"ok","timestamp":1652181583149,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":35,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"svoqtwR6sQ-t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bHFsohoWdBBT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","- Load the library and language model\n","  - Add the data to configuration pipeline of spacy nlp module \n","    - This will auto-train the data"],"metadata":{"id":"l5to3LVUeKcr"}},{"cell_type":"code","source":["# Import library\n","\n","import spacy\n","import classy_classification"],"metadata":{"id":"Bxy2a7tweKcs","executionInfo":{"status":"ok","timestamp":1652181587372,"user_tz":-330,"elapsed":337,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["# Load language model\n","\n","nlp = spacy.load('en_core_web_md')"],"metadata":{"id":"17QvrYBkeKcs","executionInfo":{"status":"ok","timestamp":1652181589760,"user_tz":-330,"elapsed":1729,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"WMo0vscReKcs","executionInfo":{"status":"ok","timestamp":1652180951709,"user_tz":-330,"elapsed":471,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":19,"outputs":[]},{"cell_type":"code","source":["# Add text categorizer pipeline with your data\n","# This may take a few minutes to train\n","\n","nlp.add_pipe(\"text_categorizer\", \n","    config={\n","        \"data\": train_class_tweets,\n","        \"model\": \"spacy\"\n","    }\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"269bd50a-fa13-4d71-9a30-a32d89629089","id":"1cpuFvrmeKct","executionInfo":{"status":"ok","timestamp":1652181594379,"user_tz":-330,"elapsed":4625,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Fitting 5 folds for each of 6 candidates, totalling 30 fits\n"]},{"output_type":"execute_result","data":{"text/plain":["<classy_classification.classifiers.spacy_internal.classySpacyInternal at 0x7ff382d6f390>"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":[""],"metadata":{"id":"-CRJ2Yc0eKcu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wBEVA4vreKcu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Testing"],"metadata":{"id":"cteW1f9seKcu"}},{"cell_type":"code","source":["class_list"],"metadata":{"id":"oOiMsIFve0PH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652181738426,"user_tz":-330,"elapsed":358,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"96522f75-7c15-46a6-f540-3eeb44a363d7"},"execution_count":39,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['anger',\n"," 'boredom',\n"," 'empty',\n"," 'enthusiasm',\n"," 'fun',\n"," 'happiness',\n"," 'hate',\n"," 'love',\n"," 'neutral',\n"," 'relief',\n"," 'sadness',\n"," 'surprise',\n"," 'worry']"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["actual_class = \"anger\"\n","tweet = test_class_tweets[actual_class][11];\n","doc = nlp(tweet)\n","predictions = doc._.cats\n","sorted_predictions = sorted(predictions.items(), key=lambda x:x[1], reverse=True)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(sorted_predictions[0][0]));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"outputId":"fb3b1387-d485-4fa8-facb-b0606ca4a91a","executionInfo":{"status":"ok","timestamp":1652181778620,"user_tz":-330,"elapsed":389,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"WUWDfWgyeKcv"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  No gas PLUS no money = Desperation!!!   and that's the word of the day!  http://twitpic.com/66zls\n","Actual class    :  anger\n","Predicted class :  anger\n"]}]},{"cell_type":"code","source":[""],"metadata":{"executionInfo":{"status":"ok","timestamp":1652181787330,"user_tz":-330,"elapsed":606,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"2aaMICsGeKcv"},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["actual_class = \"sadness\"\n","tweet = test_class_tweets[actual_class][11];\n","doc = nlp(tweet)\n","predictions = doc._.cats\n","sorted_predictions = sorted(predictions.items(), key=lambda x:x[1], reverse=True)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(sorted_predictions[0][0]));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VaeJchgxnqI4","executionInfo":{"status":"ok","timestamp":1652181849295,"user_tz":-330,"elapsed":384,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"080df1b2-f054-42aa-8af8-df4cec2d6fcb"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  Just cross 'cause I'm stuck twiddling my thumbs now, ugh\n","Actual class    :  sadness\n","Predicted class :  boredom\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"HdWE0M4PnqGL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actual_class = \"enthusiasm\"\n","tweet = test_class_tweets[actual_class][11];\n","doc = nlp(tweet)\n","predictions = doc._.cats\n","sorted_predictions = sorted(predictions.items(), key=lambda x:x[1], reverse=True)\n","\n","print(\"Sentence        :  {}\".format(tweet));\n","print(\"Actual class    :  {}\".format(actual_class));\n","print(\"Predicted class :  {}\".format(sorted_predictions[0][0]));"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"khwlUHLutZr2","executionInfo":{"status":"ok","timestamp":1652181853664,"user_tz":-330,"elapsed":365,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"31d4a828-e7df-4fa2-e4da-b29be39be82a"},"execution_count":50,"outputs":[{"output_type":"stream","name":"stdout","text":["Sentence        :  I'm missing crab legs and attending my going away instead!\n","Actual class    :  enthusiasm\n","Predicted class :  enthusiasm\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"CwPbS4sttZoR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"gfZhsqM7nqDN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extra Assignment: Create your three level sentiment classification system\n","\n","A sentiment analysis dataset, with three levels - neutral, positive and negative, can be downloaded from https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset. It is released under [CCO: Public domain license](https://creativecommons.org/publicdomain/zero/1.0/). \n","\n","Follow the steps you took for approaching the emotion detection problem statement\n","\n","**Note: Use only a small part of the dataset for training for faster results**\n","\n","**Additionally: Use preprocessing techniques to clean the dataset**"],"metadata":{"id":"GXmJIO5Inqtt"}},{"cell_type":"code","source":[""],"metadata":{"id":"i_DRPmF-nqAz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ZZEpBEnSnp9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Il4t6yM7oQ4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Extra Assignment - Fake News Detection Dataset\n","\n","Fake news is a very important problem that needs a solution. You goal is to use the dataset present at https://data.mendeley.com/datasets/zwfdmp5syg/1 to create a classifier that can detect any false news. \n","\n","**Note: Use only a small part of the dataset for training for faster results**\n","\n","**Additionally: Use preprocessing techniques to clean the dataset**"],"metadata":{"id":"1Tw1bM3AOYW-"}},{"cell_type":"code","source":["! wget https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/zwfdmp5syg-1.zip"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DhSGJzFAaLwk","outputId":"36272ec2-21f3-417d-8e9b-2c7d9f23a135"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2022-05-07 12:18:00--  https://md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com/zwfdmp5syg-1.zip\n","Resolving md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)... 52.218.121.90\n","Connecting to md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com (md-datasets-cache-zipfiles-prod.s3.eu-west-1.amazonaws.com)|52.218.121.90|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3640768 (3.5M) [application/octet-stream]\n","Saving to: ‘zwfdmp5syg-1.zip’\n","\n","zwfdmp5syg-1.zip    100%[===================>]   3.47M  1.69MB/s    in 2.1s    \n","\n","2022-05-07 12:18:03 (1.69 MB/s) - ‘zwfdmp5syg-1.zip’ saved [3640768/3640768]\n","\n"]}]},{"cell_type":"code","source":["! unzip -qq zwfdmp5syg-1.zip"],"metadata":{"id":"w_S9sXPEaLt1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","df = pd.read_excel(\"fake_new_dataset.xlsx\")"],"metadata":{"id":"KRhD2Y5waLrD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vSB1giZ7aLoQ","outputId":"02ac1876-f6b6-44a3-ab67-db6c5f908d83"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Index(['Unnamed: 0', 'title', 'text', 'subcategory', 'label'], dtype='object')"]},"metadata":{},"execution_count":70}]},{"cell_type":"code","source":[""],"metadata":{"id":"lGqgqDCjaLlG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np \n","np.unique(df[\"subcategory\"][:])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i3JkLQpCdgB7","outputId":"b684566b-a7c9-4589-a257-e8f0141b241a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['false news', 'partially false', 'true'], dtype=object)"]},"metadata":{},"execution_count":75}]},{"cell_type":"code","source":[""],"metadata":{"id":"2UcEDZSYnp60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"bxYyEApLqQaG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Or_G29-MqQXI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Weekly (Running) Assignments\n","** You will be working on these every week.\n","\n","\n","##Task 1\n","- Find 2 companies working in AI (CV, NLP) field on linkedin jobs or naukri (dot com) or any other job portal and note down what are the requirements they have.\n","- Also note down the kind of work they do and whether it interests you or not.\n","\n","*This will help you understand the kind of companies that exist in this field and where you can apply for a job*\n","\n","\n","##Task 2\n","- Read 10 AI current news, and write a summary on it in the form of a short blog. - Publish it online (preferably medium)\n","Read about 2 research labs (from universities or private) and write a synopsis on the work they do. Publish it online (preferably medium)\n","\n","*This will help you gain knowledge on the researches that are happening around the world in this space*\n"],"metadata":{"id":"jtTLSYU6IjUm"}},{"cell_type":"code","source":[""],"metadata":{"id":"hL-zprpT9w1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"T6ecP8dKqQUh"},"execution_count":null,"outputs":[]}]}