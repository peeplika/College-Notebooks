{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"L_15_Notes_+_Tutorial_+_Assignment_with_solution_Part_1.ipynb","provenance":[],"collapsed_sections":["yjov0eDW0Ozf","g8rN7FlyQV3y","CdiOQsqwO45f"],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"15yeoGWcfpy4"},"outputs":[],"source":[""]},{"cell_type":"code","source":[""],"metadata":{"id":"VAQQX5uCHg9t"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Chapter 15\n","\n","---\n","\n","\n","\n","# Introduction to Natural Language Processing\n","\n","In this chapter, you will be getting to know the basic elements in Natural Language Processing. You will be studying different topics in 5 parts, \n","- Part 1: Here, you will take text and get to its basic components using processes such as Tokenization,  Lemmatization, Stop words, Stemming, Part of speech tagging and Text Normalization\n","- Part 2: Using the basic omponents figured in the part 1 you will extract basic important words using Named entity Recognition and then learn to create Mathematical representation of texts to use them in machine learning models.\n","- Part 3: Using certain algorithms next you will explore word and grammar autocorrection, Word similarity, Key term detection and Extractive text summary\n","- Part 4: In this part you will move to training deep learning algorithms for Text sentiment analysis and Text classification, Text emotion detection\n","- Part 5: Finally, you will lean about natural language generation, training a simple chatbot and then merging computer vision and natural language processing with Image caption generation"],"metadata":{"id":"wtYEj3DEp5_t"}},{"cell_type":"code","source":[""],"metadata":{"id":"JGUPs3N6p4Py"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What lies ahead for you in this chapter\n","\n","This chapter is all about looking at text data for the first time and exploring text pre-processing algorithms\n","- Tokenization\n","- Finding out stop words\n","- Lemmatization\n","- Stemming\n","- Part of speech tagging\n","- Morphology\n","\n","And in the process you will be introduced with three different libraries\n","- Spacy\n","- NLTK\n","- TextBlob"],"metadata":{"id":"8rnT1OQJp-Yp"}},{"cell_type":"code","source":[""],"metadata":{"id":"aboauaSwHg5_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_4U9CjdT1FaA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# What is Natural Language Processing?\n","\n","It falls in the field of computer science, linguistics, and artificial intelligence that focusses on understand human language, interpret it as we do and generate response in a manner as humans do. Afterall, all the sci-fi movies have shown that the AI speaks very similar to humans. \n","\n","When we speak with each other, we understand and interpret every statement with context. For example, \n","- Statement 1: It took me two hours to travel a mere 2 kms yesterday evening\n","- Statement 2: It was horrible, my friend fell sick as well.\n","If I ask you, are these statements connected? TYou would say yes, try this with Alexa or google assistant or Siri, and check if they are able to connect the two statements and repond accordingly. \n","\n","NLP combined rules based linguistic modelling like rules of grammar, language, etc; and deep learning algorithms involving LSTMs, RNNs, etc. NLP has two parts\n","- Natural Language Understanding\n","  - Taking up human language, converting it into mathematical format, extracting meaning and key information from it.\n","- Natural Language Generation\n","  - Based on the understanding of the extracted information, the ai engine generates natural language with proper grammar and in a way we can understand it.\n","\n","\n","The basic steps in the Natural language understanding involve\n","- Lexical analysis: Splitting your text paragraph into sentences and each sentence into words.\n","- Syntactic analysis: This stage include analysis of each of these words, their meaning and their arrangements in the sentences, basically understanding the relationships\n","- Semantic analysis: This phase tries to understand meaningfulness in the sentence. \n","- Discourse integration: In this part, the context is extracted, and relationship is drawn between multiple sentences.\n","- Pragmatic analysis: This phase is a double check on intention of the text. Basically texts can have multiple meanings,\n","  - Ravi saw Suraj in front of his house with a binoculars \n","    - In this sentence, is Ravi having those binoculars or is Suraj having it. \n","  - Or is a question asked direct or rhetorical? \n","\n","\n","There are multiple applications of natural language processing\n","- Email and sms filters\n","- Smart AI chatbot assistants\n","- Making search results faster in case of huge documents\n","- Finding similarity between legal documents and cases\n","- Sentiment and emotion analysis of texts and statements\n","- Auto-summarization of large texts\n","- Text auto-correct and auto-suggest\n","... and much more!!!\n","\n","\n","\n"],"metadata":{"id":"On_PShu31FuD"}},{"cell_type":"code","source":[""],"metadata":{"id":"nTOr90ka1FXG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"HvgaWn5HHg2p"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Spacy Library\n","\n","[Spacy](https://spacy.io/) is an Industrial strength library for Natural Language Processing, released under [MIT License](https://github.com/explosion/spaCy/blob/master/LICENSE). \n","\n","![](https://drive.google.com/uc?export=view&id=1H4e3LBNkBlx8x67Is7QaCjEoeCff9XzF)\n","\n","It is one of the most used libraries in this space, developed and managed by an AI company named, [Explosion AI](https://explosion.ai/)"],"metadata":{"id":"DHg7vt3Uwg28"}},{"cell_type":"code","source":[""],"metadata":{"id":"wQ8j0jDDvp4j"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install spacy"],"metadata":{"id":"yjov0eDW0Ozf"}},{"cell_type":"code","source":["! pip install -U pip setuptools wheel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":506},"id":"iiaeCmPCvp07","executionInfo":{"status":"ok","timestamp":1652024442376,"user_tz":-330,"elapsed":16294,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"79f56cbd-3f22-40bd-f5ed-ff83cb86e41b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pip in /usr/local/lib/python3.7/dist-packages (21.1.3)\n","Collecting pip\n","  Downloading pip-22.0.4-py3-none-any.whl (2.1 MB)\n","\u001b[K     |████████████████████████████████| 2.1 MB 7.8 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (57.4.0)\n","Collecting setuptools\n","  Downloading setuptools-62.1.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 38.5 MB/s \n","\u001b[?25hRequirement already satisfied: wheel in /usr/local/lib/python3.7/dist-packages (0.37.1)\n","Installing collected packages: setuptools, pip\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pip\n","    Found existing installation: pip 21.1.3\n","    Uninstalling pip-21.1.3:\n","      Successfully uninstalled pip-21.1.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed pip-22.0.4 setuptools-62.1.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["pkg_resources"]}}},"metadata":{}}]},{"cell_type":"code","source":["! pip uninstall -y spacy"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsL_95MX0fX5","executionInfo":{"status":"ok","timestamp":1652024454521,"user_tz":-330,"elapsed":2480,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"9ac08144-ba5f-4681-9804-1702bab9dcc6"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: spacy 2.2.4\n","Uninstalling spacy-2.2.4:\n","  Successfully uninstalled spacy-2.2.4\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["! pip install spacy==3.2.4"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HVHbYRP40fTv","executionInfo":{"status":"ok","timestamp":1652024470794,"user_tz":-330,"elapsed":16286,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"515bd335-dd44-4797-c002-9ca666035a58"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting spacy==3.2.4\n","  Downloading spacy-3.2.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m53.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (1.21.6)\n","Collecting thinc<8.1.0,>=8.0.12\n","  Downloading thinc-8.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (653 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m653.3/653.3 KB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n","  Downloading typer-0.4.1-py3-none-any.whl (27 kB)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (1.0.7)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.11.3)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (21.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (0.9.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (62.1.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (0.4.1)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (7.1.2)\n","Collecting spacy-legacy<3.1.0,>=3.0.8\n","  Downloading spacy_legacy-3.0.9-py2.py3-none-any.whl (20 kB)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.0.6)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (4.64.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (2.23.0)\n","Collecting srsly<3.0.0,>=2.4.1\n","  Downloading srsly-2.4.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (457 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m457.1/457.1 KB\u001b[0m \u001b[31m39.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n","  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting pathy>=0.3.5\n","  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting catalogue<2.1.0,>=2.0.6\n","  Downloading catalogue-2.0.7-py3-none-any.whl (17 kB)\n","Collecting langcodes<4.0.0,>=3.2.0\n","  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n","  Downloading spacy_loggers-1.0.2-py3-none-any.whl (7.2 kB)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==3.2.4) (3.0.6)\n","Collecting typing-extensions<4.0.0.0,>=3.7.4\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy==3.2.4) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy==3.2.4) (3.0.8)\n","Collecting smart-open<6.0.0,>=5.0.0\n","  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==3.2.4) (2.10)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy==3.2.4) (2.0.1)\n","Installing collected packages: typing-extensions, typer, spacy-loggers, spacy-legacy, smart-open, pydantic, langcodes, catalogue, srsly, pathy, thinc, spacy\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.2.0\n","    Uninstalling typing_extensions-4.2.0:\n","      Successfully uninstalled typing_extensions-4.2.0\n","  Attempting uninstall: smart-open\n","    Found existing installation: smart-open 6.0.0\n","    Uninstalling smart-open-6.0.0:\n","      Successfully uninstalled smart-open-6.0.0\n","  Attempting uninstall: catalogue\n","    Found existing installation: catalogue 1.0.0\n","    Uninstalling catalogue-1.0.0:\n","      Successfully uninstalled catalogue-1.0.0\n","  Attempting uninstall: srsly\n","    Found existing installation: srsly 1.0.5\n","    Uninstalling srsly-1.0.5:\n","      Successfully uninstalled srsly-1.0.5\n","  Attempting uninstall: thinc\n","    Found existing installation: thinc 7.4.0\n","    Uninstalling thinc-7.4.0:\n","      Successfully uninstalled thinc-7.4.0\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\u001b[31m\n","\u001b[0mSuccessfully installed catalogue-2.0.7 langcodes-3.3.0 pathy-0.6.1 pydantic-1.8.2 smart-open-5.2.1 spacy-3.2.4 spacy-legacy-3.0.9 spacy-loggers-1.0.2 srsly-2.4.3 thinc-8.0.15 typer-0.4.1 typing-extensions-3.10.0.2\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["! python -m spacy download en_core_web_sm"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0EySGJfW0fQT","executionInfo":{"status":"ok","timestamp":1652024490310,"user_tz":-330,"elapsed":19535,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"e85d434f-9ebc-4a42-828e-26e9444ad2ab"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-sm==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.2.0/en_core_web_sm-3.2.0-py3-none-any.whl (13.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.9/13.9 MB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-sm==3.2.0) (3.2.4)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.10.0.2)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.9)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (7.1.2)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.7)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.0.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.23.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.4.3)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (8.0.15)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.3.0)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.4.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.11.3)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.7)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.8.2)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.21.6)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.6)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (62.1.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (21.3)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.6)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.9.1)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (4.64.0)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (5.2.1)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (1.24.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-sm==3.2.0) (2.0.1)\n","Installing collected packages: en-core-web-sm\n","  Attempting uninstall: en-core-web-sm\n","    Found existing installation: en-core-web-sm 2.2.5\n","    Uninstalling en-core-web-sm-2.2.5:\n","      Successfully uninstalled en-core-web-sm-2.2.5\n","Successfully installed en-core-web-sm-3.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_sm')\n"]}]},{"cell_type":"code","source":["! python -m spacy download en_core_web_md"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L4RNAPCw0fMN","executionInfo":{"status":"ok","timestamp":1652024507497,"user_tz":-330,"elapsed":17197,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"6c4d6a25-7912-4902-8c4d-0ee2bb9fadf0"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting en-core-web-md==3.2.0\n","  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.2.0/en_core_web_md-3.2.0-py3-none-any.whl (45.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.7/45.7 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: spacy<3.3.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from en-core-web-md==3.2.0) (3.2.4)\n","Requirement already satisfied: thinc<8.1.0,>=8.0.12 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (8.0.15)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (21.3)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.6)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.11.3)\n","Requirement already satisfied: typing-extensions<4.0.0.0,>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.10.0.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (62.1.0)\n","Requirement already satisfied: click<8.1.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (7.1.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.3.0)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.4.3)\n","Requirement already satisfied: blis<0.8.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.8.2)\n","Requirement already satisfied: typer<0.5.0,>=0.3.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.4.1)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.7)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.21.6)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.0.2)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.6)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.8 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.9)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.23.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (4.64.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.9.1)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.7)\n","Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.7/dist-packages (from spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (0.6.1)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from catalogue<2.1.0,>=2.0.6->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.8.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.8)\n","Requirement already satisfied: smart-open<6.0.0,>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from pathy>=0.3.5->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (5.2.1)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2021.10.8)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2->spacy<3.3.0,>=3.2.0->en-core-web-md==3.2.0) (2.0.1)\n","Installing collected packages: en-core-web-md\n","Successfully installed en-core-web-md-3.2.0\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the package via spacy.load('en_core_web_md')\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Dy-gWABV0fJc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Cevx2vOGPBHy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Natural Language ToolKit (NLTK)\n","\n","[NLTK](https://github.com/nltk/nltk) is another prominent library for understanding and intepretation of text data. It is licensed as https://github.com/nltk/nltk/blob/develop/LICENSE.txt. \n","[NLTK](https://github.com/nltk/nltk) is another prominent library for understanding and intepretation of text data. It is licensed as [Apache 2.0 License](https://github.com/nltk/nltk/blob/develop/LICENSE.txt). "],"metadata":{"id":"fXS_P1J8PBeO"}},{"cell_type":"code","source":[""],"metadata":{"id":"KH9DUMTePBEU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"_3Ikt4yjQRhP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install NLTK"],"metadata":{"id":"g8rN7FlyQV3y"}},{"cell_type":"code","source":["! pip install nltk"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9drk21oTk3Iv","outputId":"b57e19ea-e08b-4d0f-c42a-33c3d2b3c580","executionInfo":{"status":"ok","timestamp":1652006429657,"user_tz":-330,"elapsed":4093,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import nltk\n","\n","# Doanload all language corpus data and models\n","# A corpus is a collection of documents\n","nltk.download(\"all\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jTnY0u-OlVno","outputId":"334fc7e8-7460-4b81-f716-1df1e0bbd728","executionInfo":{"status":"ok","timestamp":1652006547731,"user_tz":-330,"elapsed":62386,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading collection 'all'\n","[nltk_data]    | \n","[nltk_data]    | Downloading package abc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/abc.zip.\n","[nltk_data]    | Downloading package alpino to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/alpino.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n","[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping\n","[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n","[nltk_data]    | Downloading package basque_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n","[nltk_data]    | Downloading package biocreative_ppi to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n","[nltk_data]    | Downloading package bllip_wsj_no_aux to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n","[nltk_data]    | Downloading package book_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n","[nltk_data]    | Downloading package brown to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown.zip.\n","[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n","[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n","[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n","[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/chat80.zip.\n","[nltk_data]    | Downloading package city_database to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/city_database.zip.\n","[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/cmudict.zip.\n","[nltk_data]    | Downloading package comparative_sentences to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n","[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n","[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2000.zip.\n","[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/conll2002.zip.\n","[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n","[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/crubadan.zip.\n","[nltk_data]    | Downloading package dependency_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n","[nltk_data]    | Downloading package dolch to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/dolch.zip.\n","[nltk_data]    | Downloading package europarl_raw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n","[nltk_data]    | Downloading package extended_omw to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/extended_omw.zip.\n","[nltk_data]    | Downloading package floresta to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/floresta.zip.\n","[nltk_data]    | Downloading package framenet_v15 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n","[nltk_data]    | Downloading package framenet_v17 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n","[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n","[nltk_data]    | Downloading package genesis to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/genesis.zip.\n","[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n","[nltk_data]    | Downloading package ieer to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ieer.zip.\n","[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/inaugural.zip.\n","[nltk_data]    | Downloading package indian to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/indian.zip.\n","[nltk_data]    | Downloading package jeita to /root/nltk_data...\n","[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/kimmo.zip.\n","[nltk_data]    | Downloading package knbc to /root/nltk_data...\n","[nltk_data]    | Downloading package large_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n","[nltk_data]    | Downloading package lin_thesaurus to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n","[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n","[nltk_data]    | Downloading package machado to /root/nltk_data...\n","[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n","[nltk_data]    | Downloading package maxent_ne_chunker to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n","[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n","[nltk_data]    | Downloading package moses_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/moses_sample.zip.\n","[nltk_data]    | Downloading package movie_reviews to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n","[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n","[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n","[nltk_data]    | Downloading package names to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/names.zip.\n","[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n","[nltk_data]    | Downloading package nonbreaking_prefixes to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n","[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n","[nltk_data]    | Downloading package omw to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw.zip.\n","[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/omw-1.4.zip.\n","[nltk_data]    | Downloading package opinion_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n","[nltk_data]    | Downloading package panlex_swadesh to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/paradigms.zip.\n","[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pe08.zip.\n","[nltk_data]    | Downloading package perluniprops to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping misc/perluniprops.zip.\n","[nltk_data]    | Downloading package pil to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pil.zip.\n","[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pl196x.zip.\n","[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n","[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ppattach.zip.\n","[nltk_data]    | Downloading package problem_reports to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n","[nltk_data]    | Downloading package product_reviews_1 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n","[nltk_data]    | Downloading package product_reviews_2 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n","[nltk_data]    | Downloading package propbank to /root/nltk_data...\n","[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n","[nltk_data]    | Downloading package ptb to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ptb.zip.\n","[nltk_data]    | Downloading package punkt to /root/nltk_data...\n","[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n","[nltk_data]    | Downloading package qc to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/qc.zip.\n","[nltk_data]    | Downloading package reuters to /root/nltk_data...\n","[nltk_data]    | Downloading package rslp to /root/nltk_data...\n","[nltk_data]    |   Unzipping stemmers/rslp.zip.\n","[nltk_data]    | Downloading package rte to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/rte.zip.\n","[nltk_data]    | Downloading package sample_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n","[nltk_data]    | Downloading package semcor to /root/nltk_data...\n","[nltk_data]    | Downloading package senseval to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/senseval.zip.\n","[nltk_data]    | Downloading package sentence_polarity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n","[nltk_data]    | Downloading package sentiwordnet to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n","[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n","[nltk_data]    | Downloading package sinica_treebank to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n","[nltk_data]    | Downloading package smultron to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/smultron.zip.\n","[nltk_data]    | Downloading package snowball_data to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package spanish_grammars to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n","[nltk_data]    | Downloading package state_union to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/state_union.zip.\n","[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/stopwords.zip.\n","[nltk_data]    | Downloading package subjectivity to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n","[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/swadesh.zip.\n","[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/switchboard.zip.\n","[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n","[nltk_data]    |   Unzipping help/tagsets.zip.\n","[nltk_data]    | Downloading package timit to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/timit.zip.\n","[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/toolbox.zip.\n","[nltk_data]    | Downloading package treebank to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/treebank.zip.\n","[nltk_data]    | Downloading package twitter_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n","[nltk_data]    | Downloading package udhr to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr.zip.\n","[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/udhr2.zip.\n","[nltk_data]    | Downloading package unicode_samples to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n","[nltk_data]    | Downloading package universal_tagset to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n","[nltk_data]    | Downloading package universal_treebanks_v20 to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package vader_lexicon to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet.zip.\n","[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n","[nltk_data]    | Downloading package webtext to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/webtext.zip.\n","[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n","[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n","[nltk_data]    | Downloading package word2vec_sample to\n","[nltk_data]    |     /root/nltk_data...\n","[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n","[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet.zip.\n","[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet2021.zip.\n","[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet31.zip.\n","[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n","[nltk_data]    | Downloading package words to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/words.zip.\n","[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n","[nltk_data]    |   Unzipping corpora/ycoe.zip.\n","[nltk_data]    | \n","[nltk_data]  Done downloading collection all\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":[""],"metadata":{"id":"Mu8Ke-1RN_D8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"xOivjIaWQqs_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# TextBlob Library\n","\n","[Textblob](https://github.com/sloria/TextBlob) is an opensource library for processing of text data licensed as [MIT License](https://github.com/sloria/TextBlob/blob/dev/LICENSE). \n","\n"],"metadata":{"id":"J6wJHexxN_dl"}},{"cell_type":"code","source":[""],"metadata":{"id":"66RDhku6N_A1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Install TextBlob Library"],"metadata":{"id":"CdiOQsqwO45f"}},{"cell_type":"code","source":["! pip install textblob"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FtiHGQx2N-9c","executionInfo":{"status":"ok","timestamp":1652006559823,"user_tz":-330,"elapsed":4331,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"a8e34a89-c74b-4992-868d-2235857d6ade"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: textblob in /usr/local/lib/python3.7/dist-packages (0.15.3)\n","Requirement already satisfied: nltk>=3.1 in /usr/local/lib/python3.7/dist-packages (from textblob) (3.2.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.1->textblob) (1.15.0)\n","\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"XQh1wb9E076o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"zikNQa-0Q24i"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Tokenization\n","\n","When working with images, you tokenize images into pixels, or set of neighboring pixels. Similarly, in nlp, it is the process of splitting text into sentences and further into words or even further into characters. Tokenization could even mean pair of words. \n","\n","![](https://drive.google.com/uc?export=view&id=1BpLBDmtrV7DBXDnqaaxgwemKyuj9cncQ)"],"metadata":{"id":"ZoQbg65R1ASw"}},{"cell_type":"code","source":[""],"metadata":{"id":"avzHoS5T073O"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JFigHfZk07zm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using spacy"],"metadata":{"id":"UUs4wYl0Q-r1"}},{"cell_type":"code","source":["# Import the libraery\n","\n","import spacy"],"metadata":{"id":"TLrrpyaHhlhn","executionInfo":{"status":"ok","timestamp":1652005510176,"user_tz":-330,"elapsed":5664,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Load the language details and models\n","\n","nlp = spacy.load(\"en_core_web_sm\")"],"metadata":{"id":"WSiyD0CDMqfF","executionInfo":{"status":"ok","timestamp":1652005514176,"user_tz":-330,"elapsed":598,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["sentence = \"This is a sentence. This is the second one\";"],"metadata":{"id":"O4w9s72YiDws","executionInfo":{"status":"ok","timestamp":1652005597962,"user_tz":-330,"elapsed":8,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Load the sentence and process it through loaded spacy module\n","\n","doc = nlp(sentence)"],"metadata":{"id":"44BTz4A9hleH","executionInfo":{"status":"ok","timestamp":1652005599393,"user_tz":-330,"elapsed":6,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["# Print the tokens\n","\n","for token in doc:\n","    print(token.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"btGSwZlhhla5","outputId":"55b446a2-108e-4cdf-dcb5-14d80ce3e3f1","executionInfo":{"status":"ok","timestamp":1652005600014,"user_tz":-330,"elapsed":9,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["This\n","is\n","a\n","sentence\n",".\n","This\n","is\n","the\n","second\n","one\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"D7F1WKGI07w9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ibkL5C_o07t2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using textblob"],"metadata":{"id":"_3Ks9HmsRCUm"}},{"cell_type":"code","source":[""],"metadata":{"id":"ogxVLIWxN677"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from textblob import TextBlob"],"metadata":{"id":"aCZxvtIr0RCR","executionInfo":{"status":"ok","timestamp":1652006640484,"user_tz":-330,"elapsed":513,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["sentence = \"This is a sentence. This is the second one\";\n","blob = TextBlob(sentence)"],"metadata":{"id":"CtGDlxpb0Q_c","executionInfo":{"status":"ok","timestamp":1652006660962,"user_tz":-330,"elapsed":412,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["# Tokenize to sentences\n","blob.sentences"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2ihoyrIj0v-P","outputId":"29f93b01-a0c5-4a39-c346-4e79ff5ca623","executionInfo":{"status":"ok","timestamp":1652006677913,"user_tz":-330,"elapsed":360,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Sentence(\"This is a sentence.\"), Sentence(\"This is the second one\")]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":["# Tokenize to words\n","blob.words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EnjGcsRe0v7n","outputId":"5bcab078-ae5f-4540-ae6c-e7a5f16b021c","executionInfo":{"status":"ok","timestamp":1652006688516,"user_tz":-330,"elapsed":13,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["WordList(['This', 'is', 'a', 'sentence', 'This', 'is', 'the', 'second', 'one'])"]},"metadata":{},"execution_count":19}]},{"cell_type":"code","source":[""],"metadata":{"id":"Z3LQq3wb07rV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reference material\n","\n","Read the following articles and papers\n","\n","- https://www.analyticsvidhya.com/blog/2020/05/what-is-tokenization-nlp/\n","- https://nlp.stanford.edu/IR-book/html/htmledition/tokenization-1.html"],"metadata":{"id":"OoIokEsY2iXa"}},{"cell_type":"code","source":[""],"metadata":{"id":"awAL1sr12h_t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6y8utarpvpxw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Detecting stop words\n","\n","Stop words are those words which tend to give least amount of information from the sentence. There is usally a stop word list in every list, but there is no need to have a strict rule that stop words in one text need to be stop words in other texts as well.\n","\n","These include words such as \"is\", \"this\", \"and\", etc"],"metadata":{"id":"8tFCvCgHRb99"}},{"cell_type":"code","source":[""],"metadata":{"id":"47aSugMeRajP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Continuing with spacy"],"metadata":{"id":"DBhgqvdVSZj3"}},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.is_stop)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22WKvXLeSXh4","executionInfo":{"status":"ok","timestamp":1652006968441,"user_tz":-330,"elapsed":382,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"1179203a-93b3-4623-da13-a706b4c97213"},"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["This True\n","is True\n","a True\n","sentence False\n",". False\n","This True\n","is True\n","the True\n","second False\n","one True\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"G0Ko232WSXfW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence2 = \"You are now starting to learn Natural Language Processing and intend to study it for 2 weeks\";"],"metadata":{"id":"2PnCB7voSjgW","executionInfo":{"status":"ok","timestamp":1652007003386,"user_tz":-330,"elapsed":370,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["doc2 = nlp(sentence2)"],"metadata":{"id":"JzI3E-fmSjgX","executionInfo":{"status":"ok","timestamp":1652007003975,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["for token in doc2:\n","    print(token.text, token.is_stop)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"330fm62PRagR","executionInfo":{"status":"ok","timestamp":1652007018823,"user_tz":-330,"elapsed":385,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"08e9d6d8-5baf-4490-8bca-e79abc64e75a"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["You True\n","are True\n","now True\n","starting False\n","to True\n","learn False\n","Natural False\n","Language False\n","Processing False\n","and True\n","intend False\n","to True\n","study False\n","it True\n","for True\n","2 False\n","weeks False\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"vLrqlPNVSmze"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reference material\n","\n","Read the following articles and papers\n","\n","- https://medium.com/@saitejaponugoti/stop-words-in-nlp-5b248dadad47\n","- https://towardsdatascience.com/text-pre-processing-stop-words-removal-using-different-libraries-f20bac19929a"],"metadata":{"id":"9dIvolok3Ixy"}},{"cell_type":"code","source":[""],"metadata":{"id":"bGTLpZe528cc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"LrgG4T1fS_L1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Extract numbers from text\n","\n","Number share out a lot of information too. It is a good practice in text preprocessing to extract out numberical data"],"metadata":{"id":"bSQbs8rcS_os"}},{"cell_type":"code","source":[""],"metadata":{"id":"rPdgUsWrSmvn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Continuing with spacy"],"metadata":{"id":"r6QNmCpDTUTR"}},{"cell_type":"code","source":[""],"metadata":{"id":"W6xoUjP9TVrG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KRw8OdHZVuBy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"The box had some 5 diamonds each worth 100000$. They were stolen by a team of 7.\";"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652008012553,"user_tz":-330,"elapsed":346,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"bYQX8Sm8Vx7K"},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["doc = nlp(sentence)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652008013131,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"CdvJIKfPVx7K"},"execution_count":35,"outputs":[]},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.is_alpha)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1652008017679,"user_tz":-330,"elapsed":11,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"fea70744-c3bf-4b34-b95a-08a792b2961a","id":"LdzIlHb8Vx7K"},"execution_count":36,"outputs":[{"output_type":"stream","name":"stdout","text":["The True\n","box True\n","had True\n","some True\n","5 False\n","diamonds True\n","each True\n","worth True\n","100000$. False\n","They True\n","were True\n","stolen True\n","by True\n","a True\n","team True\n","of True\n","7 False\n",". False\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"Mc-MdgyAVt6d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"w-NA8wVWWkpb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Stemming\n","\n","It is the process of reducing a word to its stem, in other words it removes out a few last characters from the word. It improves the accuracy of information extraction "],"metadata":{"id":"WKcl4xYQWn39"}},{"cell_type":"code","source":[""],"metadata":{"id":"6ogP4bi_WkmT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using NLTK\n","\n","You will be using [porter stemming algorithm](https://www.geeksforgeeks.org/introduction-to-stemming/) which removes the morphological endings of words. "],"metadata":{"id":"wxOBJWZZhdS3"}},{"cell_type":"code","source":[""],"metadata":{"id":"VBJ7IyjVWki_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","sentence = \"Ravi spent years caring for his sick father. His father got cured in a span of 4 years and is now fully healthy.\"\n","\n","words = word_tokenize(sentence)"],"metadata":{"id":"mR_3C57gk3Fy","executionInfo":{"status":"ok","timestamp":1652008862733,"user_tz":-330,"elapsed":374,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"sdGYCyY4k3CO","executionInfo":{"status":"ok","timestamp":1652008865879,"user_tz":-330,"elapsed":8,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":37,"outputs":[]},{"cell_type":"code","source":["from nltk.stem.porter import PorterStemmer\n","\n","stemmed = [PorterStemmer().stem(w) for w in words]"],"metadata":{"id":"RAGp8ujGk3AC","executionInfo":{"status":"ok","timestamp":1652008869706,"user_tz":-330,"elapsed":391,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":38,"outputs":[]},{"cell_type":"code","source":["for i in range(len(words)):\n","    print(words[i], stemmed[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B4zAOlYikP8F","outputId":"0960b845-d1d5-45ca-a6dd-eade91c18733","executionInfo":{"status":"ok","timestamp":1652010536059,"user_tz":-330,"elapsed":419,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Ravi ravi\n","spent spent\n","years year\n","caring care\n","for for\n","his hi\n","sick sick\n","father father\n",". .\n","His hi\n","father father\n","got got\n","cured cure\n","in in\n","a a\n","span span\n","of of\n","4 4\n","years year\n","and and\n","is is\n","now now\n","fully fulli\n","healthy healthi\n",". .\n"]}]},{"cell_type":"markdown","source":["\"Cured\" gets to its roots \"cure\"\n","\n","\"years\" to \"year\"\n","\n","But, \"fully\" got to \"fulli\"?, \"his\" got to \"hi\"?\n","\n"],"metadata":{"id":"aQkjGPQuhIsl"}},{"cell_type":"code","source":[""],"metadata":{"id":"pBYTGwVQWkgD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reference material\n","\n","Read the following articles and papers\n","\n","- https://towardsdatascience.com/stemming-of-words-in-natural-language-processing-what-is-it-41a33e8996e2\n","- https://www.analyticsvidhya.com/blog/2021/11/an-introduction-to-stemming-in-natural-language-processing/\n","- https://www.geeksforgeeks.org/introduction-to-stemming/"],"metadata":{"id":"y1-WjQU23P-z"}},{"cell_type":"code","source":[""],"metadata":{"id":"tbwHfPwm3Pks"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"JWq84PfuZ2V4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Lemmatization\n","\n","A better way to achieve some form of stemming is using lemmatization, basically getting words to its true roots. "],"metadata":{"id":"8TUEMDnrhZlX"}},{"cell_type":"markdown","source":["## Using Spacy"],"metadata":{"id":"QCvPDjP5iJPM"}},{"cell_type":"code","source":["sentence = \"Ravi spent years caring for his sick father. His father got cured in a span of 4 years and is now fully healthy.\""],"metadata":{"id":"4at3UguEZ2Tg","executionInfo":{"status":"ok","timestamp":1652011329272,"user_tz":-330,"elapsed":345,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","source":["doc = nlp(sentence)"],"metadata":{"id":"SfSS1mftZ2Qw","executionInfo":{"status":"ok","timestamp":1652011330803,"user_tz":-330,"elapsed":6,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":42,"outputs":[]},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.lemma_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6SNhZD3HjAWA","executionInfo":{"status":"ok","timestamp":1652011332408,"user_tz":-330,"elapsed":7,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"1cf8b694-9b8f-4c38-f04b-93ec9d8eb912"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Ravi Ravi\n","spent spend\n","years year\n","caring care\n","for for\n","his his\n","sick sick\n","father father\n",". .\n","His his\n","father father\n","got got\n","cured cure\n","in in\n","a a\n","span span\n","of of\n","4 4\n","years year\n","and and\n","is be\n","now now\n","fully fully\n","healthy healthy\n",". .\n"]}]},{"cell_type":"markdown","source":["\"Cured\" gets to its roots \"cure\"\n","\n","\"Caring\" to \"care\"\n","\n","\"years\" to \"year\"\n","\n","But, \"fully\" and \"his\" remained the same."],"metadata":{"id":"F51ajcngjMpU"}},{"cell_type":"code","source":[""],"metadata":{"id":"l0-qzS3ojATH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reference material\n","\n","Read the following articles and papers\n","\n","- https://nlp.stanford.edu/IR-book/html/htmledition/stemming-and-lemmatization-1.html\n","- https://www.machinelearningplus.com/nlp/lemmatization-examples-python/\n","- https://www.datacamp.com/community/tutorials/stemming-lemmatization-python"],"metadata":{"id":"K_SYZJBK3nTU"}},{"cell_type":"code","source":[""],"metadata":{"id":"SdVoAnve3mQ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"T2IQ0qbhjAQT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"NFS4SVPTjlk7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Part of speech Tagging\n","\n","Let's get to some grammar basics, we have the following parts of speech \n","- Noun\n","- Verb\n","- Adjective\n","- Adverb\n","- Pronoun\n","- Punctiation\n","\n","\n","Spacy does a better job, adds more parts of speech\n"," - Adposition\n"," - Auxiliary\n"," - Conjunction\n"," - Determiner\n"," - Interjection\n"," - Propernoun\n","\n","Part of speech tagging is important to extract trees \n"],"metadata":{"id":"SQifDkcDjl9a"}},{"cell_type":"code","source":[""],"metadata":{"id":"DKuAoHynjlhq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"wDYQMfuXm4Rc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using spacy"],"metadata":{"id":"oyRaGrzBnD6z"}},{"cell_type":"code","source":["sentence = \"You are now starting to learn Natural Language Processing and intend to study it for 2 weeks\";"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652012377600,"user_tz":-330,"elapsed":383,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"IXsA6yutm4my"},"execution_count":44,"outputs":[]},{"cell_type":"code","source":["doc = nlp(sentence)"],"metadata":{"executionInfo":{"status":"ok","timestamp":1652012379458,"user_tz":-330,"elapsed":10,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"id":"5XX6GDikm4my"},"execution_count":45,"outputs":[]},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.pos_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SF6pJUlPmBPu","executionInfo":{"status":"ok","timestamp":1652012380818,"user_tz":-330,"elapsed":10,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"3a9631f6-6f55-45b2-b414-efc010644d8c"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["You PRON\n","are AUX\n","now ADV\n","starting VERB\n","to PART\n","learn VERB\n","Natural PROPN\n","Language PROPN\n","Processing PROPN\n","and CCONJ\n","intend VERB\n","to PART\n","study VERB\n","it PRON\n","for ADP\n","2 NUM\n","weeks NOUN\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"M1Z7cDgNmBw-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Also allows dependency detection, find how two words in the sentence are connected. To understand each dependencies, use \n","\n","```\n","spacy.explain(\"<dependency>\")\n","```\n","\n","You can read about dependency grammar and parsing here -  \n","https://www.analyticsvidhya.com/blog/2021/12/dependency-parsing-in-natural-language-processing-with-examples/"],"metadata":{"id":"FrVrMn1OpLTD"}},{"cell_type":"code","source":["for token in doc:\n","    print(token.text, token.dep_)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fo5Wox2nmoLR","outputId":"926bb8b3-87d9-4053-9a47-ed844c3c4fda","executionInfo":{"status":"ok","timestamp":1652012982748,"user_tz":-330,"elapsed":467,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["You nsubj\n","are aux\n","now advmod\n","starting ROOT\n","to aux\n","learn xcomp\n","Natural compound\n","Language compound\n","Processing dobj\n","and cc\n","intend conj\n","to aux\n","study xcomp\n","it dobj\n","for prep\n","2 nummod\n","weeks pobj\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"il1LN7iZpcJX","executionInfo":{"status":"ok","timestamp":1652012666957,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":48,"outputs":[]},{"cell_type":"code","source":["from spacy import displacy"],"metadata":{"id":"DYVszAqVpcGQ","executionInfo":{"status":"ok","timestamp":1652012791397,"user_tz":-330,"elapsed":415,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":49,"outputs":[]},{"cell_type":"code","source":["displacy.render(doc, style='dep', jupyter=True, options={'distance': 90})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":403},"id":"EBt2gr2NpcDC","outputId":"5682ed29-75e0-47f0-b972-2ceef0b0048d","executionInfo":{"status":"ok","timestamp":1652012792834,"user_tz":-330,"elapsed":9,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":50,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"d6480c534a2a40269c759f6310ff7182-0\" class=\"displacy\" width=\"1580\" height=\"362.0\" direction=\"ltr\" style=\"max-width: none; height: 362.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">You</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"140\">are</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"140\">AUX</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"230\">now</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"230\">ADV</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"320\">starting</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"320\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"500\">learn</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"500\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"590\">Natural</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"590\">PROPN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"680\">Language</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"680\">PROPN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">Processing</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"860\">and</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"860\">CCONJ</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"950\">intend</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"950\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1040\">to</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1040\">PART</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">study</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">VERB</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1220\">it</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1220\">PRON</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1310\">for</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1310\">ADP</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1400\">2</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1400\">NUM</tspan>\n","</text>\n","\n","<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"272.0\">\n","    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">weeks</tspan>\n","    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n","</text>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-0\" stroke-width=\"2px\" d=\"M70,227.0 C70,92.0 310.0,92.0 310.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M70,229.0 L62,217.0 78,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-1\" stroke-width=\"2px\" d=\"M160,227.0 C160,137.0 305.0,137.0 305.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M160,229.0 L152,217.0 168,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-2\" stroke-width=\"2px\" d=\"M250,227.0 C250,182.0 300.0,182.0 300.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">advmod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M250,229.0 L242,217.0 258,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-3\" stroke-width=\"2px\" d=\"M430,227.0 C430,182.0 480.0,182.0 480.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M430,229.0 L422,217.0 438,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-4\" stroke-width=\"2px\" d=\"M340,227.0 C340,137.0 485.0,137.0 485.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M485.0,229.0 L493.0,217.0 477.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-5\" stroke-width=\"2px\" d=\"M610,227.0 C610,182.0 660.0,182.0 660.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M610,229.0 L602,217.0 618,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-6\" stroke-width=\"2px\" d=\"M700,227.0 C700,182.0 750.0,182.0 750.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M700,229.0 L692,217.0 708,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-7\" stroke-width=\"2px\" d=\"M520,227.0 C520,92.0 760.0,92.0 760.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M760.0,229.0 L768.0,217.0 752.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-8\" stroke-width=\"2px\" d=\"M520,227.0 C520,47.0 855.0,47.0 855.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M855.0,229.0 L863.0,217.0 847.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-9\" stroke-width=\"2px\" d=\"M520,227.0 C520,2.0 950.0,2.0 950.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M950.0,229.0 L958.0,217.0 942.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-10\" stroke-width=\"2px\" d=\"M1060,227.0 C1060,182.0 1110.0,182.0 1110.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">aux</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1060,229.0 L1052,217.0 1068,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-11\" stroke-width=\"2px\" d=\"M970,227.0 C970,137.0 1115.0,137.0 1115.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">xcomp</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1115.0,229.0 L1123.0,217.0 1107.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-12\" stroke-width=\"2px\" d=\"M1150,227.0 C1150,182.0 1200.0,182.0 1200.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1200.0,229.0 L1208.0,217.0 1192.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-13\" stroke-width=\"2px\" d=\"M1150,227.0 C1150,137.0 1295.0,137.0 1295.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1295.0,229.0 L1303.0,217.0 1287.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-14\" stroke-width=\"2px\" d=\"M1420,227.0 C1420,182.0 1470.0,182.0 1470.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nummod</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1420,229.0 L1412,217.0 1428,217.0\" fill=\"currentColor\"/>\n","</g>\n","\n","<g class=\"displacy-arrow\">\n","    <path class=\"displacy-arc\" id=\"arrow-d6480c534a2a40269c759f6310ff7182-0-15\" stroke-width=\"2px\" d=\"M1330,227.0 C1330,137.0 1475.0,137.0 1475.0,227.0\" fill=\"none\" stroke=\"currentColor\"/>\n","    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n","        <textPath xlink:href=\"#arrow-d6480c534a2a40269c759f6310ff7182-0-15\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n","    </text>\n","    <path class=\"displacy-arrowhead\" d=\"M1475.0,229.0 L1483.0,217.0 1467.0,217.0\" fill=\"currentColor\"/>\n","</g>\n","</svg></span>"]},"metadata":{}}]},{"cell_type":"code","source":[""],"metadata":{"id":"6Y9FTRC2mBuL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Zzr_xGkAmBpa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Using textblob"],"metadata":{"id":"clFEvhDixynQ"}},{"cell_type":"code","source":["sentence = \"You are now starting to learn Natural Language Processing and intend to study it for 2 weeks\";"],"metadata":{"id":"tpVfGHVWyBPi","executionInfo":{"status":"ok","timestamp":1652013071126,"user_tz":-330,"elapsed":443,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["pos = TextBlob(sentence)"],"metadata":{"id":"Xi-uttbzyBM9","executionInfo":{"status":"ok","timestamp":1652013073359,"user_tz":-330,"elapsed":380,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":54,"outputs":[]},{"cell_type":"code","source":["pos.tags"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOjWeinryBJ4","outputId":"204cc9d0-0b29-4041-9424-8e9a6b5c05b8","executionInfo":{"status":"ok","timestamp":1652013075618,"user_tz":-330,"elapsed":579,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":55,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('You', 'PRP'),\n"," ('are', 'VBP'),\n"," ('now', 'RB'),\n"," ('starting', 'VBG'),\n"," ('to', 'TO'),\n"," ('learn', 'VB'),\n"," ('Natural', 'NNP'),\n"," ('Language', 'NNP'),\n"," ('Processing', 'NNP'),\n"," ('and', 'CC'),\n"," ('intend', 'VBP'),\n"," ('to', 'TO'),\n"," ('study', 'VB'),\n"," ('it', 'PRP'),\n"," ('for', 'IN'),\n"," ('2', 'CD'),\n"," ('weeks', 'NNS')]"]},"metadata":{},"execution_count":55}]},{"cell_type":"code","source":[""],"metadata":{"id":"7cGzPnm1mBm9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"9WoWPP103_Vq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Reference material\n","\n","Read the following articles and papers\n","\n","- https://towardsdatascience.com/natural-language-processing-dependency-parsing-cf094bbbe3f7\n","- https://web.stanford.edu/~jurafsky/slp3/14.pdf\n","- http://nlpprogress.com/english/dependency_parsing.html\n"],"metadata":{"id":"dLul_vYA3_o6"}},{"cell_type":"code","source":[""],"metadata":{"id":"0XQ3MbVX3_NT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Ufs58GRkp2ec"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Assignment: Text Normalization\n","\n","The steps involved in text normalization are (Note: The steps need not be the same always nor the order)\n","- Convert text to all upper case or lower case\n","- Remove all numerical data if isn't important, else keep it (optional)\n","- Tokenize data\n","- Remove whitespaces and punctuations\n","- Stem or Lemmatize data"],"metadata":{"id":"Xbi_Zj8RqN_F"}},{"cell_type":"code","source":[""],"metadata":{"id":"I41HU77Rp2bF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KkiCAZYxp2Wr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sentence = \"You are now starting to learn Natural Language Processing. You intend to study its for 2 weeks. Instead of spending 100$ on an online course you decided to study it using free material.\";"],"metadata":{"id":"vLVNokEDy84V","executionInfo":{"status":"ok","timestamp":1652024572388,"user_tz":-330,"elapsed":469,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"KiuGP3C6y81O","executionInfo":{"status":"ok","timestamp":1652024573904,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Import spacy and load the language model"],"metadata":{"id":"yeVueMSOzx7d","executionInfo":{"status":"ok","timestamp":1652024574300,"user_tz":-330,"elapsed":11,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["import spacy\n","nlp = spacy.load('en_core_web_sm')"],"metadata":{"id":"I9Myr7gmzuRX","executionInfo":{"status":"ok","timestamp":1652024579965,"user_tz":-330,"elapsed":5674,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"6WlAG7bQzuOb","executionInfo":{"status":"ok","timestamp":1652024585346,"user_tz":-330,"elapsed":527,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["# Load the sentence"],"metadata":{"id":"VAi_fL0ey8yO","executionInfo":{"status":"ok","timestamp":1652024585940,"user_tz":-330,"elapsed":7,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["doc = nlp(sentence)"],"metadata":{"id":"g1NWPPJ9z-IN","executionInfo":{"status":"ok","timestamp":1652024585942,"user_tz":-330,"elapsed":7,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"mdGF_jP10eQG","executionInfo":{"status":"ok","timestamp":1652024587234,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["# Tokenize the sentence"],"metadata":{"id":"KxX24Tel0eNL","executionInfo":{"status":"ok","timestamp":1652024587235,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["words = [];\n","for token in doc:\n","    words.append(token)"],"metadata":{"id":"4NDZXw6-0gp9","executionInfo":{"status":"ok","timestamp":1652024587711,"user_tz":-330,"elapsed":2,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":11,"outputs":[]},{"cell_type":"code","source":["words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5rVJVkwJ0vkb","executionInfo":{"status":"ok","timestamp":1652024588274,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"928fc1fe-a864-47cd-e7c7-0de00943e7fa"},"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[You,\n"," are,\n"," now,\n"," starting,\n"," to,\n"," learn,\n"," Natural,\n"," Language,\n"," Processing,\n"," .,\n"," You,\n"," intend,\n"," to,\n"," study,\n"," its,\n"," for,\n"," 2,\n"," weeks,\n"," .,\n"," Instead,\n"," of,\n"," spending,\n"," 100,\n"," $,\n"," on,\n"," an,\n"," online,\n"," course,\n"," you,\n"," decided,\n"," to,\n"," study,\n"," it,\n"," using,\n"," free,\n"," material,\n"," .]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":[""],"metadata":{"id":"KivgKq_lz-Eo","executionInfo":{"status":"ok","timestamp":1652024589603,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":12,"outputs":[]},{"cell_type":"code","source":["# Need not remove the numericals\n","# Detect and remove stop words"],"metadata":{"id":"X3u2x9nVz-Bu","executionInfo":{"status":"ok","timestamp":1652024589604,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["filtered_words_1 = [];\n","for token in words:\n","    if not token.is_stop:\n","        filtered_words_1.append(token)"],"metadata":{"id":"LeXAV3Nk0Afx","executionInfo":{"status":"ok","timestamp":1652024589605,"user_tz":-330,"elapsed":5,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["filtered_words_1"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mC9ribVX0AcL","executionInfo":{"status":"ok","timestamp":1652024590272,"user_tz":-330,"elapsed":8,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"e744d60f-dec0-4fd1-9444-cd685b9dcac1"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[starting,\n"," learn,\n"," Natural,\n"," Language,\n"," Processing,\n"," .,\n"," intend,\n"," study,\n"," 2,\n"," weeks,\n"," .,\n"," Instead,\n"," spending,\n"," 100,\n"," $,\n"," online,\n"," course,\n"," decided,\n"," study,\n"," free,\n"," material,\n"," .]"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":[""],"metadata":{"id":"4Ug-k9jW0AWY","executionInfo":{"status":"ok","timestamp":1652024591617,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["# Remove punctuation and whitespaces"],"metadata":{"id":"MXDx303r1hoh","executionInfo":{"status":"ok","timestamp":1652024591618,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["filtered_words_2 = [];\n","punctuations=[\"?\", \",\", \":\", \"!\", \".\", \";\", \"\\\"\", \"'\"];\n","for token in filtered_words_1:\n","    if token.text not in punctuations:\n","        filtered_words_2.append(token);"],"metadata":{"id":"-DLYQ_as1hkp","executionInfo":{"status":"ok","timestamp":1652024592294,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["filtered_words_2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FSvD2h901o8T","executionInfo":{"status":"ok","timestamp":1652024592294,"user_tz":-330,"elapsed":3,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"34c64449-5965-412f-c035-08b89988a90d"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[starting,\n"," learn,\n"," Natural,\n"," Language,\n"," Processing,\n"," intend,\n"," study,\n"," 2,\n"," weeks,\n"," Instead,\n"," spending,\n"," 100,\n"," $,\n"," online,\n"," course,\n"," decided,\n"," study,\n"," free,\n"," material]"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","source":[""],"metadata":{"id":"QIpeRZoX1o5b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply lemmatization\n","# Get everything to lowercase"],"metadata":{"id":"Id1RMPOsU_81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["normalized_words = [];\n","for token in filtered_words_2:\n","    normalized_words.append(token.lemma_.lower());"],"metadata":{"id":"sGBPYJnYU_5q","executionInfo":{"status":"ok","timestamp":1652024699648,"user_tz":-330,"elapsed":8,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}}},"execution_count":21,"outputs":[]},{"cell_type":"code","source":["normalized_words"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3Q-F57B9U_2j","executionInfo":{"status":"ok","timestamp":1652024709809,"user_tz":-330,"elapsed":4,"user":{"displayName":"AI IIOT6","userId":"13315000152944417255"}},"outputId":"5965092d-f96b-4bb0-bf60-0c2f405e6a7a"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['start',\n"," 'learn',\n"," 'natural',\n"," 'language',\n"," 'processing',\n"," 'intend',\n"," 'study',\n"," '2',\n"," 'week',\n"," 'instead',\n"," 'spend',\n"," '100',\n"," '$',\n"," 'online',\n"," 'course',\n"," 'decide',\n"," 'study',\n"," 'free',\n"," 'material']"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":[""],"metadata":{"id":"T4NhpXk_1o17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"7PFm7L4FWJ8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"OSLXSqh7WJ5s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Weekly (Running) Assignments\n","** You will be working on these every week.\n","\n","\n","##Task 1\n","- Find 2 companies working in AI (CV, NLP) field on linkedin jobs or naukri (dot com) or any other job portal and note down what are the requirements they have.\n","- Also note down the kind of work they do and whether it interests you or not.\n","\n","*This will help you understand the kind of companies that exist in this field and where you can apply for a job*\n","\n","\n","##Task 2\n","- Read 10 AI current news, and write a summary on it in the form of a short blog. - Publish it online (preferably medium)\n","Read about 2 research labs (from universities or private) and write a synopsis on the work they do. Publish it online (preferably medium)\n","\n","*This will help you gain knowledge on the researches that are happening around the world in this space*\n"],"metadata":{"id":"jtTLSYU6IjUm"}},{"cell_type":"code","source":[""],"metadata":{"id":"s5yBprTTJlnA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"iA6rewBC1oy7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"Lw8zptdi1hh8"},"execution_count":null,"outputs":[]}]}